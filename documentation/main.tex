\documentclass[11pt]{article}
\author{Laura Zharmukhametova, Nada Amin, Will Byrd}
\title{Staged miniKanren}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{scalerel}
\usepackage{hyperref} 
\usepackage{xcolor} 
\usepackage{multicol}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{ marvosym }

%\usepackage{pstricks, pst-node, pst-plot}

\newcommand{\ds}{\displaystyle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\A}{\mathbb{A}}

\usepackage[margin=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{mathrsfs}

\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem*{proposition}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\renewcommand*{\proofname}{Proof} 
\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\usepackage{mathtools}      

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Laura Zharmukhametova, Nada Amin, Will Byrd}
\rhead{Staged miniKanren}

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  % keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  % stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\usepackage[subdued,defaultmathsizes]{mathastext}
\MTnonlettersobeymathxx     % math alphabets will act on (, ), [, ], etc...
\MTexplicitbracesobeymathxx % math alphabets will act on \{ and \}
\MTfamily {\ttdefault}      % we will declare a math version using tt font
\Mathastext [typewriter]    % the math version is called typewriter

\begin{document}
\normalsize
\baselineskip=1.5\baselineskip
\section{Staged miniKanren}
\tab Staged miniKanren is an extension of miniKanren that supports staging. Staged programming is manual partial evaluation, where offline manual binding time analysis is used to select operations for lifting. In the case of staged miniKanren, we typically choose to lift some unifications and some conditionals.\newline 
\tab Given a program and only partially known input data, one can eliminate or simplify certain parts of the code, by performing computations that only involve available data. Depending on the known input, we might also be able to make observations such as whether a loop is degenerate for most iterations and restructure the code accordingly. As a result, we can have a new program that is specialized to the available input. \newline 
\tab In order to perform as much computation as possible, it is important to correctly annotate arguments according to their availability. This is done via \textit{binding time analysis}. \newline 
\tab The power of staging is manual control of binding time analysis, where one can manually decide which operations are to be lifted. The manual design stems from the observation that automatic binding-time analysis is hard, and that manual control is a powerful compromise that allows the staging user to specify what they want exactly.

\subsection{Deferring unifications}
\tab In staged miniKanren, some unifications are done in the first stage, while others are quoted out and gets deferred to the second stage. The second stage represents code that is "kept for later", while the first stage is for executing now. Deferring a unification is similar to deferring a command in functional programming. \newline 
\tab To defer a unification, we use the $\texttt{l==}$ operator:
\begin{lstlisting}[]
(test (run* (q) (l== q 1) (l== q 2))
      '((_.0 !! ((== _.0 2) (== _.0 1)))))
(test (run* (q) (conde [(l== q 1)] [(l== q 2)]))
      '((_.0 !! ((== _.0 1))) (_.0 !! ((== _.0 2)))))
\end{lstlisting}
\tab Here, the two unifications get deferred to the second stage. To defer a goal we use the $\texttt{lift}$ operator:
\begin{lstlisting}[]
(test (run* (q) (lift `(conde [(== ,q 1)] [(== ,q 2)])))
      '((_.0 !! ((conde ((== _.0 1)) ((== _.0 2)))))))
\end{lstlisting}
\tab Lifting unifications $\texttt{l==}$ is defined in terms of the more general $\texttt{lift}$.
\begin{lstlisting}[]
(define fake-evalo (lambda (q n)
                     (fresh ()
                       (l== q n)
                       (l== n n))))
(test
 (run* (q)
        (fresh (c1 c2)
                (lift-scope (fake-evalo q 1) c1)
              (lift-scope (fake-evalo q 2) c2)
              (lift `(conde ,c1 ,c2))))
 '((_.0 !!
        ((conde
          ((== _.0 '1) (== '1 '1))
          ((== _.0 '2) (== '2 '2)))))))
\end{lstlisting}
\subsection{Dynamic variables}
\tab An alternative approach to manual lifting of unifications is to introduce dynamic variables, where we annotate some variables by hand, and the lifting automatically follows. To decide whether the unification needs to be deferred we could examine the terms being unified to see whether they contain a dynamic variable. This approach is not very promising as it is unclear how dynamic variables should interact with non-dynamic variables and the rest of the code. For example, if we just say that a term containing a dynamic variable is dynamic, then an expression like `(cdr (cons 5 ,y))` where only `y` is dynamic will be treated as dynamic, which is not ideal.
\subsection{Using staged miniKanren for fuzzing}
\tab Fuzzing is used for automated software testing via random high-frequency generation of programs (for testing interpeters and compilers) as well as inputs of a certain format (for testing programs in general). Suppose we have a program whose input is a binary search tree; it is meant to accept binary search trees and reject any other input. A fuzzer for this program would automatically generate trees to test that inputs are accepted and rejected correctly. In general, if we wish to test for soundness, we would like to avoid generating completely irrelevant and incorrect inputs, as the software is probably good enough to reject them \cite{dewey2015fuzzing}. It is more likely that almost correct, or ``quasi''-correct inputs will be mistakenly accepted. Thus, ideally, a fuzzer should generate inputs that are, for example, somehow ``off by one''. In miniKanren, a given acceptor function can be compiled into a relation, and then use the $\code{run}$ function to fuzz correct inputs. It would be useful to have an infrastructure that takes an arbitrary acceptor relation and makes a relation for fuzzing quasi correct inputs. Consider these examples.

\begin{enumerate}[(a)]
    \item \textit{Mirror trees.} Suppose we have a reverse function that recursively swaps the leaves of a tree. Then define a \textit{mirror} to be a binary tree that is equal to its reverse. Now we can try to define \textit{almost-mirrors} that are almost symmetrical. To fuzz almost-mirrors, we can inject a random choice somewhere in the definition of mirror:
    \begin{lstlisting}
    (run 1 (q) 
    (evalo 
        `(letrec ((mirror 
                (lambda (t)
                    (match t 
                        [(? number? n) n]
                        [`(node ,l ,(? number? n) ,r)
                            (list `node (choice (mirror r) r) n (choice (mirror l) l))]))))
        (equal? ',q (mirror `,q)))
        #t))
    \end{lstlisting}
    where $\code{choice}$ chooses the first argument with probability $90 \%$, for example. Here $\code{mirror}$ is a function that takes a tree $\code{t}$ and returns $\mathtt{\#t}$ if $\code{t}$ is a mirror. The goal $\code{evalo}$ takes the $\code{letrec}$ expression setting $\code{mirror}$ to the recursive mirror function in $\code{(equal? ',q (mirror `,q))}$ and unifies it with $\# t$, in an empty substitution.\newline 
    \tab One way to do this without weights is to have a non-deterministic $\code{choice}$ implemented using $\code{conde}$:
    \begin{lstlisting}
    (define (choice-primo expr env val)
        (fresh (e2 e3)
            (== `(choice ,e2 ,e3) expr)
            (not-in-envo 'choice env)
            (conde
                ((eval-expo e2 env val))
                ((eval-expo e3 env val)))))
    \end{lstlisting}

    \item \textit{Binary search trees.} An quasi binary search tree could be a binary tree where the left leaf of some node is greater than the right one by one. 
    \item \textit{A program processing HTML pages} can be fuzzed with quasi HTML pages containing subtle syntax mistakes.
\end{enumerate}
\tab A potential use case for staged miniKanren is using it for optimizing fuzzers. Thus, the unavailable data in the first example could be the first argument to the $\code{run}$ function and the argument to $\code{evalo}$ so that we stage the same program with some known parameters replaced by symbols:
    \begin{lstlisting}
    (run X (q) 
    (evalo 
        `(letrec ((mirror 
                (lambda (t)
                    (match t 
                        [(? Y n) n]
                        [`(node ,l ,(? Y n) ,r)
                            (list `node (choice (mirror r) r) n (choice (mirror l) l))]))))
        (equal? ',q (mirror `,q)))
    #t))
    \end{lstlisting}
Where we aim to get an optimized fuzzer specialized to generating almost-mirrors, and the $Y$ argument to the resulting function is the type of data stored in the tree.\newline 
\tab In general, we wish to explore how we can use staging for optimizing functions into relations. Thus, given a program of the form 
    \begin{lstlisting}
    (run X (q) 
    (evalo 
        `(letrec ((f ...))
        ...)
    ...))
    \end{lstlisting}
    \tab Where some values are symbolic, we might be able
    to lift operations containing them and generate an optimized program.
\subsection{Different types of variables and values}
\tab In addition to usual lexical variables in Scheme which can be created using $\code{let}$, $\lambda$, etc, miniKanren has its own logic variables, which can be bound to values via a $\textit{substitution}$ mapping. These can be 
\textit{ground}, i.e. associated with a value, or \textit{fresh}. If we unify two fresh variables $\code{X}$ and $\code{Y}$, they both remain fresh. A term is \textit{ground} if it contains variables that are all ground. Logic variables are represented by a vector and can be created using the $\code{fresh}$ operator. In particular, the $\code{fresh}$ operator 
creates a fresh logic variable and binds it to a lexical variable. Initially, the logic variable does not have a value associated with it, but it can become ground through unification. In addition to associating logic variables with values, we can map them to constraints such as disequality and absento.

\subsection{Deferring operations}
\tab Staging seems to be related to delayed goals, and so it might be potentially useful to integrate the two in a single miniKanren extension. This integration would look as follows. When compiling functions into relations, starting with a conjuction, we evaluate the first goal and the result is incorporated in each substitution, then we move on to the next goal, etc. Now in addition to these we could have an extra step of deferring trigonometric, logarithmic and other tricky unifications, such as $\code{(== X (cos }{\frac{\pi}{4}\code{))}}$ or $\code{(== X (+ 3} \hspace{0.3cm}\code{Z))}$ where $\code{X, Z}$ will be known later. Unifying $\code{x}$ with $\texttt{cos} \hspace{0.1cm}\pi$ is thus delayed until we have either more information (from a new unification) or appropriate tools for performing, for example, floating-point computations. Even if we have ground arguments we can have various reasons to delay a unification, for example we might want to unify $\code{X}$ with precisely $\sqrt{2}$ and not its floating-point representation. Dependency analysis would be necessary for the case when we wish to perform the unification as soon as there is more information available.

Then, to make it possible to enter and exit a $\code{run}$ or a $\code{run}*$  process at flexible points we might wish to enable feeding and retrieving a constraint stream from it. 


\section{Current implementation of Staged miniKanren}
\subsection{Staged interpreter}
\tab Suppose our goal is to interpret an expression $\code{expr}$ and unify the result with $\code{val}$. This can be done with $\code{(evalo expr val)}$, which calls $\code{eval-expo}$ with an initial environment.
The staged version of $\code{eval-expo}$ needs to handle staged code. Consider the lifting operator $\code{lift}$. The result of applying $\code{lift}$ to a goal is a lifted goal (so like a goal, it takes a state and returns a state.) In the code below, the first two goals are ``$\code{stage?}$ is true'' and  ``$\code{expr}$ is a variable''. The third goal takes a state $\code{c}$ and returns the state resulting from applying $$\code{(lift `(u-eval-expo ,expr ,(quasi (walk* env (c->S c))) ,val))}$$ to $\code{c}$. In particular, we defer calling the unstaged $\code{eval-expo}$ on the variable $\code{expr}$ and $\code{val}$. Note that the environment now is a substitution map, since the unstaged $\code{eval-expo}$ takes a substitution as the second argument:
\begin{lstlisting}
(define (eval-expo stage? expr env val)
 ...
  (conde
    ((== stage? #t) (varo expr)
     (lambda (c)
       ((lift `(u-eval-expo ,expr ,(quasi (walk* env (c->S c))) ,val))
        c)))
\end{lstlisting}
\tab Another lifting operator is $\code{l==}$, the lifted unification constructor. Below a fresh variable $\code{v}$ is introduced and unified with $\code{expr}$. The unification of $\code{v}$ with $\code{val}$ is then deferred via $\mathtt{l==}$, unlike the first-stage unification $\code{(== expr v)}$ that we had in the unstaged miniKanren:
\begin{lstlisting}
(define (eval-expo stage? expr env val)
  (conde
    ...    
    ((conde
       ((non-varo expr))
       ((== stage? #f)))
     (conde
       ((fresh (v)
          (== `(quote ,v) expr)
          (absento 'closure v)
          (absento 'prim v)
          (not-in-envo 'quote env)
          ((if stage? l== ==) val v)))
\end{lstlisting}
In the unstaged miniKanren, if $\code{expr}$ is a number, we unify it with $\code{val}$: 
\begin{lstlisting}
    (define (eval-expo expr env val)	
        ...
        (conde
            ((numbero expr) (== expr val))
            ...))
\end{lstlisting} 
In staged miniKanren, $\code{eval-expo}$ takes the additional $\code{staged?}$ argument. If $\code{staged?}$ is true, then we defer the unification: 
\begin{lstlisting}
       ((numbero expr) ((if stage? l== ==) expr val))
\end{lstlisting}
Similarly, if it is a symbol, then depending on the value of $\code{staged?}$, we either look up the symbol immediately or defer to the second stage: 
\begin{lstlisting}
       ((symbolo expr) (lookupo stage? expr env val))
\end{lstlisting}
where $\code{lookupo}$ is the function
\begin{lstlisting}
(define (lookupo stage? x env t)
  (fresh (y b rest)
    (== `((,y . ,b) . ,rest) env)
    (conde
      ((== x y)
       (conde
         ((fresh (v) (== `(val . ,v) b) (== v t)))
         ((fresh (rec-fold? lam-expr)
            (== `(rec ,rec-fold? . ,lam-expr) b)
            (conde
              ((== rec-fold? #t) (== `(call ,x) t))
              ((== rec-fold? #f) (== `(closure ,lam-expr ,env) t)))))))
      ((=/= x y)
       (lookupo stage? x rest t)))))
\end{lstlisting}
\tab If $\code{expr}$ is a function (which we check by unifying $\code{`(lambda ,x ,body)}$ with $\code{expr}$ in the first stage) and $\code{stage?}$ is true, then we defer the unification of $\code{val}$ with the closure $\code{`(closure (lambda ,x, }$ $\code{body), env)}$ to the second stage:
\begin{lstlisting}
        ((fresh (x body)
            (== `(lambda ,x ,body) expr)
            ((if stage? l== ==) `(closure (lambda ,x ,body) ,env) val)
\end{lstlisting}
\tab The rest of this clause is the same as in the unstaged miniKanren. The variable $\code{x}$ can either be variadic:
% question why does symbolo mean variadic?
% (lambda (x . foo) …) the rest of the arguments go to foo
% (lambda args …)

\begin{lstlisting}
            (conde
                ((symbolo x))
\end{lstlisting}
or multi-argument:
\begin{lstlisting}
                ((list-of-symbolso x)))
\end{lstlisting}
\tab Finally, we check that $\code{`lambda}$ is not in the environment:
\begin{lstlisting}
            (not-in-envo 'lambda env)))
\end{lstlisting}
\tab Here, the expression is an operation, so we unify $\code{expr}$ with $`(,rator . ,rands)$, where $\code{rator}$ stands for ``operator'' and $\code{rands}$ stands for ``operands'':
\begin{lstlisting}
       ((fresh (rator x rands body env^ a* res)
          (== `(,rator . ,rands) expr)
\end{lstlisting}
\tab Then we have the goal that $\code{x}$ is a symbol that is a variadic argument.
\begin{lstlisting}
          (symbolo x)
\end{lstlisting}
\tab We also need an environment $\code{res}$ in which the body of the operator will be evaluated. This is the environment in the closure $\code{rator}$, concatenated with the association of the symbol $\code{x}$ with the pair $\code{val}$ and $\code{a*}$, where $\code{a*}$ is the result of evaluating the operands $\code{rands}$. The operator is a relation, so we pass $\code{val}$ as an argument in addition to the $\code{rands}$. 
\begin{lstlisting}
          (== `((,x . (val . ,a*)) . ,env^) res)
\end{lstlisting}
\tab The operator is a lambda taking $\code{x}$ and returning $\code{body}$. In particular, we evaluate unstaged $\code{rator}$ and unify it with a closure.
\begin{lstlisting}
          (eval-expo #f rator env `(closure (lambda ,x ,body) ,env^))
\end{lstlisting}
\tab We then evaluate the body of the operator, by calling $\code{eval-expo}$ on $\code{body}$, passing $\code{stage?}$ argument, and the $\code{val}$ argument. Note that we evaluate $\code{rator}$ with the $\code{stage?}$ argument set to false, as here we do not defer evaluating the operator itself; the staging applies to evaluating the body of $\code{rator}$. Thus, we do pass the original $\code{stage?}$ argument when we evaluate the \textit{body} of the operator in the extended environment:
\begin{lstlisting}
          (eval-expo stage? body res val)
\end{lstlisting}
\tab The operands are evaluated using $\code{eval-listo}$ and are unified with $\code{a*}$.
\begin{lstlisting}
          (eval-listo rands env a*)))
\end{lstlisting}
\tab If the argument to the operand is a list, we have similar goals but use $\code{ext-env*o}$ to extend the environment \code{env\^}:
\begin{lstlisting}
       ((fresh (rator x* rands body env^ a* res)
          (== `(,rator . ,rands) expr)
          (eval-expo #f rator env `(closure (lambda ,x* ,body) ,env^))
          (eval-listo rands env a*)
          (ext-env*o x* a* env^ res)
          (eval-expo stage? body res val)))
\end{lstlisting}
%question: why are we swapping x and args 
\tab If $\code{stage?}$ is true, and $\code{rator}$ evaluates to some $\code{\`(call ,p-name)}$, then we can lift the goal $\code{(,p-name . ,a*) ,val}$. Here $\code{a*}$ is the list of the evaluated $\code{rands}$ and $\code{val}$ is unified the result of the operation.
% question: what does p-name stand for?
\begin{lstlisting}
       ((fresh (rator rands a* p-name)
          (== stage? #t)
          (== `(,rator . ,rands) expr)
          (eval-expo #f rator env `(call ,p-name))
          (eval-listo rands env a*)
          (lift `((,p-name . ,a*) ,val))))
\end{lstlisting}
\tab If $\code{rator}$ is a symbol and $\code{stage?}$ is true, then we do not evaluate $\code{rands}$ and unify with an $\code{a*}$. The unstaged evaluation of the entire $\code{expr}$ is lifted, and we pass $\code{(quasi (walk* env (c->S c)))}$ as the environment to $\code{u-eval-expo}$ (``unstaged $\code{eval-expo}$'').
\begin{lstlisting}         
       ((fresh (rator rands p-name)
            (== stage? #t)
            (== `(,rator . ,rands) expr)
            (symbolo rator)
            (eval-expo #f rator env `(sym . ,p-name))
            (lambda (c)
              ((lift `(u-eval-expo ',expr ,(quasi (walk* env (c->S c))) ,val))
               c))))
\end{lstlisting}
\tab If $\code{rator}$ is a primitive operator, we can find its id by unifying with $\code{(prim . ,prim-id)}$ and evaluate it on the evaluated operands $\code{a*}$.
\begin{lstlisting}               
       ((fresh (rator x* rands a* prim-id)
          (== `(,rator . ,rands) expr)
          (eval-expo #f rator env `(prim . ,prim-id))
          (eval-primo prim-id a* val)
          (eval-listo rands env a*)))
\end{lstlisting}
\tab Here we unify $\code{expr}$ with a $\code{letrec}$ expression. The $\code{bindings*}$ fresh variable unifies with the bindings in the expression and $\code{letrec-body}$ unifies with the body.
\begin{lstlisting}       
       ((fresh (bindings* letrec-body out-bindings* env^)
          (== `(letrec ,bindings*
                 ,letrec-body)
              expr)
\end{lstlisting}
\tab The $\code{bindings*}$ are evaluated using $\code{letrec-bindings-evalo}$ with the result being $\code{out-bindings*}$. We unify $\code{stage?}$ with true and use $\code{lift-scope}$ to generate staged code in the body of the $\code{letrec}$ expression. Then the new $\code{letrec}$ expression $\code{(letrec ,out-bindings* (fresh () . ,c-letrec-body))}$ is lifted using the $\code{lift}$ operator.
\begin{lstlisting}
          (letrec-bindings-evalo bindings* out-bindings* env env^ env^)
          (not-in-envo 'letrec env)
          (== stage? #t)
          (fresh (c-letrec-body)
            (lift-scope
             (eval-expo #t letrec-body env^ val)
             c-letrec-body)
            (lift `(letrec ,out-bindings*
                     (fresh () . ,c-letrec-body))))))
        ...
       ))))
\end{lstlisting}
\tab Additionally, the staged interpreter has these functions.\newline 
\tab \textit{The} $\code{mapo}$ \textit{function} unifies $\code{xs}$ with a pair $\code{xa}$ and $\code{xd}$, unify $\code{ys}$ with a pair $\code{ya}$ and $\code{yd}$. The function  $\code{fo}$ is applied with the arguments $\code{xa}$ and $\code{ya}$, and we apply the $\code{mapo}$ relation to the tails of the two lists.
\begin{lstlisting}[]
(define (mapo fo xs ys)
  (conde
    ((== xs '()) (== ys '()))
    ((fresh (xa xd ya yd)
       (== xs (cons xa xd))
       (== ys (cons ya yd))
       (fo xa ya)
       (mapo fo xd yd)))))
\end{lstlisting}
\tab The $\code{mapo}$ function is used to make a list of symbols. 
\begin{lstlisting}
(define (make-list-of-symso xs ys)
  (mapo (lambda (x y) (== y (cons 'sym x))) xs ys))
\end{lstlisting}
\tab Goals for checking that $\code{x}$ is a variable or a non-variable are introduced:
\begin{lstlisting}[]
(define (varo x)
  (lambda (c)
    (if (var? (walk* x (c->S c)))
        c
        #f)))
(define (non-varo x)
  (lambda (c)
    (if (var? (walk* x (c->S c)))
        #f
        c)))
\end{lstlisting}
\tab The $\code{fix-l==}$ function lifts $\code{t}$ if $\code{(car t)}$ is $\code{==}$ or $\code{=/=}$.
\begin{lstlisting}
(define fix-l==
  (lambda (t)
    (if (and (pair? t)
             (or (eq? '== (car t))
                 (eq? '=/= (car t))))
        (list (car t) (quasi (cadr t)) (quasi (caddr t)))
        t)))
\end{lstlisting}
\tab \textit{The} $\code{quasi}$ \textit{function}. If $\code{t}$ is a variable, return that variable. If $\code{t}$ is a pair and the first entry is a symbol, return the second entry, otherwise apply $\code{quasi}$ to both entries. If $\code{t}$ is null, return $\code{'()}$. Otherwise $\code{(list 'quote t)}$ is returned.
\begin{lstlisting}
(define quasi
  (lambda (t)
    (cond
      ((var? t) t)
      ((and (pair? t) (eq? (car t) 'sym)) (cdr t))
      ((pair? t) (list 'cons (quasi (car t)) (quasi (cdr t))))
      ((null? t) ''())
      (else (list 'quote t)))))
\end{lstlisting}
\tab \textit{The function} $\code{walk-lift}$ walks $\code{C}$ and lifts all unifications.
\begin{lstlisting}
(define walk-lift
  (lambda (C S)
    (map fix-l== (walk* (reverse C) S))))
\end{lstlisting}
\tab \textit{Lifting a goal} adds the goal to the $\code{C}$ constraint store. 
\begin{lstlisting}
(define lift
  (lambda (x)
    (lambdag@ (c : S D A T C L)
      `(,S ,D ,A ,T ,(cons x C) ,L))))
\end{lstlisting}
\textit{Lifting a scope.} This function takes a goal $\code{g}$ and a variables $\code{var}$ and makes another goal, which, when applied it to a state $\code{c}$, applies $\code{g}$ to a modified $\code{c}$ where the $\code{C}$ store is empty.
\begin{lstlisting}
(define lift-scope
  (lambda (g out)
    (lambdag@ (c : S D A T C L)
      (bind*
       (g `(,S ,D ,A ,T () ,L))
       (lambdag@ (c2 : S2 D2 A2 T2 C2 L2)
         ((fresh ()
            (== out (walk-lift C2 S2)))
          `(,S ,D ,A ,T ,C ,L)))))))
\end{lstlisting}
\tab \textit{The} $\code{l==}$ \textit{operator} takes arguments $\mathtt{e_1}$ and $\mathtt{e_2}$ and creates a lifted goal $\code{(lift `(== ,e1 ,e2))}$. 
% question: why do we need fresh here??
\begin{lstlisting}
(define l== (lambda (e1 e2) (fresh () (lift `(== ,e1 ,e2)))))
\end{lstlisting}
\tab \textit{The} $\code{dynamic}$ \textit{function} takes a symbol $\code{xs}$ and returns a goal that, given a state, appends it to the $\code{L}$ field.
\begin{lstlisting}
(define dynamic
  (lambda xs
    (lambdag@ (c : S D A T C L)
      `(,S ,D ,A ,T ,C ,(append L xs)))))
\end{lstlisting}
\subsection{Staged miniKanren}
In the canonical implementation of miniKanren, a \textit{state} $\code{c}$ is an object containing \begin{itemize}
    \item The \textit{substitution} mapping $\code{S}$ used to hold substitutions resulting from unifications $\code{==}$. The substitution is accessed by $\code{c->S}$.
    \item The \textit{disequality} mapping $\code{D}$ used to keep track of disequalities associated with variables. It is accessed by the function $\code{c->D}$. 
    \item The \textit{absento} mapping $\code{A}$ used to handle the $\code{absento}$ goals and accessed by $\code{c->A}$. 
    \item The \textit{types} mapping $\code{T}$ used to keep track of constraints such as $\code{symbolo}$ and $\code{numbero}$. 
\end{itemize} 
In staged miniKanren, the state object has two additional components, accessed by $\code{c->C}$ and $\code{c->L}$ respectively:
\begin{itemize}
    \item The \textit{code} component $\code{C}$ is a list of deferred goals. 
    \item The $\code{L}$ component holding dynamic variables.
\end{itemize}
The $\code{C}$ component holds second stage goals and is extended whenever we use a lifting operator such as $\code{lift}, \code{lift-scope}$ or $\code{l==}$. \newline 
\tab The $\code{lconde}$ macro is for lifting $\code{conde}$ expressions. We let $\code{r}$ be the list consisting of the results of applying all clauses to $\code{c2}$, where $\code{c2}$ is the same as $\code{c}$ except the $\code{C}$ list is empty.
\begin{lstlisting}
(define-syntax lconde
  (syntax-rules ()
    ((_ (g0 g ...) (g1 g^ ...) ...)
     (lambdag@ (c : S D A T C L)
       (let ((r (let ((c2 `(,S ,D ,A ,T () ,L)))
                  (append (all-of (bind* (g0 c2) g ...))
                          (all-of (bind* (g1 c2) g^ ...)) ...))))
         ((lift `(conde ,@(map (lambda (c3) (walk-lift (c->C c3) (c->S c3))) r))) c))))))
\end{lstlisting}
The $\code{walk-lift}$ replaces variables in $\code{C}$ with their values according to the substitution $\code{S}$:
\begin{lstlisting}
(define walk-lift
  (lambda (C S)
    (map fix-l== (walk* (reverse C) S))))
\end{lstlisting}
where $\code{walk*}$ finds the values of all variables in a list and returns a list with the found substitutions:
\begin{lstlisting}
(define walk*
  (lambda (v S)
    (case-value (walk v S)
      ((x) x)
      ((av dv)
       (cons (walk* av S) (walk* dv S)))
      ((v) v))))
\end{lstlisting}
The $\code{walk}$ function finds the value of a variable in a substitution:
\begin{lstlisting}
(define walk
  (lambda (u S)
    (cond
      ((and (var? u) (assq u S)) =>
       (lambda (pr) (walk (rhs pr) S)))
      (else u))))
\end{lstlisting}
The $\code{bind}$ function binds goals by consecutively applying goals to a state. The $\code{bind*}$ function applies goals to a stream of states:
\begin{lstlisting}
(define-syntax bind*
  (syntax-rules ()
    ((_ e) e)
    ((_ e g0 g ...) (bind* (bind e g0) g ...))))
(define bind
  (lambda (c-inf g)
    (case-inf c-inf
      (() (mzero))
      ((f) (inc (bind (f) g)))
      ((c) (g c))
      ((c f) (mplus (g c) (lambdaf@ () (bind (f) g)))))))
\end{lstlisting}
% question: $\code{c-inf}$ is for $c$-infimum?
The $\code{post-unify-=/=}$ function is used in the definition of the $\code{=/=}$ goal constructor:
\begin{lstlisting}
(define =/= 
  (lambda (u v)
    (lambdag@ (c : S D A T)
      (cond
        ((unify u v S) => (post-unify-=/= S D A T))
        (else (unit c))))))
\end{lstlisting}
Here even though the goal is a disequality between $\code{u}$ and $\code{v}$, these are first unified in the $\code{S}$ store. Then, the prefix of the resulting substitution $\code{S+}$ is taken to be a disequality extension $\code{D+}$ to $\code{D}$.  In particular, we need to subsume redundancy among constraints $\code{A}, \code{T}$ and $\code{D}$. For example, if $x$ is absent in $y$ then $x$ is not equal to it, so disequality between $x$ and $y$ holds automatically. Similarly, a number is not a symbol, etc.
In the unstaged miniKanren, these subsumptions are done with the $\code{post-unify}$ function:
\begin{lstlisting}
(define post-unify-=/=	
  (lambda (S D A T)	
    (lambda (S+)	
      (cond	
        ((eq? S+ S) (mzero))	
        (else (let ((D+ (list (prefix-S S+ S))))	
                (let ((D+ (subsume A D+)))	
                  (let ((D+ (subsume T D+)))	
                    (let ((D (append D+ D)))	
                      (unit `(,S ,D ,A ,T)))))))))))
\end{lstlisting} 
In the staged miniKanren, we can additionally handle disequality between dynamic variables. In particular, we can partition the new disequalities according into two sets. Let $\code{C+D-}$ be this partition. The first set in $\code{C+D-}$ contains the constraints where one of the variables is dynamic (hence the $\code{C+}$ part in the name). The second set doesn't contain dynamic variables. Now we can take the dynamic set and write all pairs in it as disequality goals again. The resulting deferred code is then appended to the $\code{C}$ (second-stage code) component of the state:
\begin{lstlisting}
(define post-unify-=/=
  (lambda (S D A T C L)
    ...
	                (let ((dynamic? (lambda (x) (memq x L))))
                      (let ((C+D- (partition (lambda (v)
                                               (or (dynamic? (car v))
                                                   (dynamic? (cdr v))))
                                             (apply append D+))))
                        (let ((C+ (map (lambda (v) `(=/= ,(car v) ,(cdr v)))
                                       (car C+D-))))
                          (let ((D (append D+ D)))
                            (let ((C (append (reverse C+) C)))
                              (unit `(,S ,D ,A ,T ,C ,L)))))))))))))))
\end{lstlisting}
The disequality constraints involving dynamic variables can thus be deferred to the second stage. \newline 
% question: what is subsume used for?
\tab \textit{Subsumed constraints}.
It's possible that a constraint, say $A$ is satisfied whenever some other constraint $B$ is. In this case, the constraint $B$ \textit{subsumes} the constraint $A$ \cite{byrd2010relational}. \newline 
\tab The gen function.
This function turns a functional procedure into a relational one
\begin{lstlisting}
(define gen
  (lambda (p-name inputs rhs . contexts)
\end{lstlisting}
The $\code{gen}$ function takes the procedure name, inputs, the right-hand-side and contexts. The result is a relational procedure taking the inputs and an additional $\code{out}$ argument.
\begin{lstlisting}
    (let ((context (if (null? contexts) (lambda (x) x) (car contexts))))
      (let ((r (run 1 (q)
                  (fresh (env inputs^)
                    (ext-env*o inputs inputs^ initial-env env)
                    (make-list-of-symso inputs inputs^)
                    (eval-expo #t
                               (context
                                `(letrec ((,p-name (lambda ,inputs ,rhs)))
                                   (,p-name . ,inputs)))
                               env
                               q)))))
        (let ((r (car r)))
          (fix-scope
           `(lambda (,@inputs out)
              (fresh ()
                (== ,(car r) out)
                . ,(caddr r)))))))))
\end{lstlisting}
\section{Related work: mixed computation and partial evaluation}
\subsection{Mathematical premise of partial evaluation}
\tab A mathematical premise behind partial evaluation is a theorem stating that there exists a computable function that maps from recursive partial functions of two variables to recursive partial functions of one variable via 
A \textit{recursive function} is any function that can be computed by a Turing machine. A \textit{recursive partial function} is a partial function that is also recursive.\newline 
\tab Suppose we enumerate all Turing machines, so that $P_x$ is the Turing machine with index $x$ (which is called the \textit{G\"{o}del} number of $P_x$). Then let $\phi_x^{(k)}$ denote the partial function of $k$ variables computed by $P_x$. Then we have the following theorem \cite{rogers1987theory}.
\begin{theorem}[Kleene's s-m-n theorem]
Let $m, n \ge 1$ and fix $x, y_1,\ldots ,y_m$. Then there is a recursive function $s_n^m$ of $m+1$ variables such that for all $x, y_1,\ldots y_m$,
$$\lambda z_1, \ldots ,z_n [\phi_x^{(m+n)}(y_1,\ldots y_m, z_1,\ldots, z_n)] = \phi_{s_n^m(x, y_1,\ldots, y_m)}^{(n)}.$$
\end{theorem}
\begin{proof}
\tab Suppose $m = n = 1$. We have a class of all possible partial functions of one variable, sending $z\mapsto \lambda z [\phi_x^{(2)}(y, z)]$ for some $x, y$. This can be viewed as a formal characterization for a class recursive partial functions of one variable. By Basic Result in \cite{rogers1987theory}, we can have a procedure $f$ for going from this characterization of functions to the original characterization used to enumerate all Turing machines.  Then, by Church's Thesis, there exists a recursive function $f$ of two variables such that $$ \lambda z [\phi_x^{(2)}(y, z)] = \phi_{f(x,y)}.$$ We can then set $s_1$ to be the function $f$. Church's Thesis is the generally accepted proposal that Turing and Church characterizations of algorithms agree with the intuitive notions of algorithm and computable functions.
\end{proof}
\subsection{Original works by Ershov}
\tab Staged programming can be understood as manual partial evaluation, which is the idea that even the most efficient algorithm can be made faster if we specialize it to certain inputs. It was developed by Futamura \cite{futamura1983partial} in the 70-s. Around the same time, Ershov \cite{ershov1982mixed} and his students explored a related idea of mixed computation. He described it as the process of jointly transforming programs and data in a partially specified setting in order to increase efficiency. \newline 
\tab In the first works \cite{ershov1982mixed}, the formal definition was as follows: a mixed computer was defined as a software processor that receives as input some representation of the program and part of the input data and receives at the output a transformed program, part of the result and data that require additional processing. Ershov distinguished functional and operational approaches to this problem. In the functional approach, we are given a function $\code{f}$ of variables $\code{x, y}$. Let us instantiate $x$ with value $v_x$, and let $g$ be the function $\mathtt{y \mapsto f(v_x, y)}$. A $\textit{mixed computation}$ is then a process for deriving a program $P_{v_x}$, called a $\textit{projection}$ of $\code{P}$ onto $\code{x}$, for computing $\code{g}$. We call this kind of computation mixed because concrete computation involving $\code{x}$ is mixed with code generation. In the operational approach, our starting point is a program $P$ composed of elementary steps. These steps are partitioned into disjoint sets $A$ and $B$, where $A$ are $\textit{permissible}$ steps and $B$ are $\textit{suspensible}$ steps. Then by $\textit{mixed computation}$ we mean computing the permissible steps and generating a $\textit{residual}$ program from the set $B$. \newline 
\tab A notable application of these ideas is that we can treat programs and even languages themselves as input data. Formally, let us represent an implementation language $L$ by a domain $\textbf{D}$, a set of programs  $\textbf{P}$, and a semantics $V$, so $L = (\textbf{D}, \textbf{P}, V)$. Assume that $\textbf{D}$ has a free component $\textbf{X}$ and a bound component $\textbf{Y}$, so that we can project programs onto the $\textbf{X}$ component. Let $l = (\Xi, \Pi, \sigma)$ be a source language, where $\Xi$ is the data domain, $\Pi$ is the set of all programs written in $l$ and $\sigma$ is semantics, so that, for all $\pi \in \Pi$ and $\xi \in \Xi$, $$ \sigma(\pi, \xi) = \pi(\xi).$$ Then one can use projections to approach the following problems:
\begin{enumerate}[(a)]
    \item \textit{Source language $l = (\Xi, \Pi, \sigma)$, program $\pi\in \Pi$ are known, input $\xi \in \Xi$ is unknown}. Thus, $l, \pi \in X$ and $\xi \in Y$ and we wish to compile $\pi$ into $obj \in \textbf{P}$ such that $obj$ agrees with $\pi$ on all inputs $\xi \in \Xi$.
    \item \textit{Source language $l$ is known, program $\pi$ is unknown}. Thus, $l\in X$ and $\pi \in Y$, and we are looking for a compiler $comp \in \textbf{P}$ such that $comp(\pi)(\xi) = obj (\xi)$ for all $\xi \in \Xi$.
    \item \textit{Source language $l$, program $\pi$ and input $\xi$ are all unknown}. We are then looking for a compiler making compilers, i.e. a program $c \in \textbf{P}$ such that $c(\sigma)(\pi)(\xi) = comp(\pi)(\xi)$ for all $\sigma, \pi, \xi$.
\end{enumerate}
 An interpreter $\textbf{int}\in \textbf{P}$ for $l$ computes $\pi(\xi)$ i.e. $\textbf{int}(\pi, \xi) = \sigma(\pi, \xi)$.
\tab Define also $\textbf{mix}$ to be a program that computes the projection of $p\in \textbf{P}$ onto known inputs $x\in \textbf{X}$, so $$\textbf{mix}(p, x, \textbf{Y}) = V(\text{mix}, (p, x, \textbf{Y}) = p_x (\textbf{Y}).$$
 \tab Then $$p(x, y) = p_x(y).$$ From the above equations we can derive that 
 $$\textbf{mix}(\textbf{int}, \pi, \Sigma) = \textbf{int}_{\pi} (\Sigma),$$ and $$\textbf{int}_{\pi}(\Sigma) = ob(\Sigma),$$ i.e. to compile a program we project the interpreter onto the source code.
 \tab We also have $$\textbf{mix}_{\textbf{int}}(\Pi, \Sigma) = comp(\Pi, \Sigma),$$ i.e. to make a compiler we project $\textbf{mix}$ onto the interpreter.
 \tab For the third projection, we have $$\textbf{mix}_{\textbf{mix}}(\textbf{int}, \Pi, \Sigma) = \textbf{mix}(\textbf{mix}, \textbf{int}, (\Pi, \Sigma)) = \textbf{mix}_{\textbf{int}}(\Pi, \Sigma)) = comp(\pi, \Sigma),$$
 i.e. to make a compiler-compiler we project $\textbf{mix}$ onto itself.
 \subsection{Monovariant and Polyvariant binding time analysis}
\tab Automated binding time analysis can be monovariant and polyvariant. The general idea is as follows: a monovariant binding time analyzer declares a variable dynamic if it is dynamic in at least one calculation. We would like to transform the original program to make more computations available by copying both the program code and the data as needed. Consider this fragment of the interpreter for evaluating expressions \cite{bulyonkov1993extracting}:
\begin{lstlisting}[]
    (define (eval expr mem)
        (cond 
            ((isConst? expr) (fetch-constant expr))
            ((isVar? expr) (lookup (fetch-Var expr) mem))
            ((isOp? expr)
                (apply-Op
                    (fetch-Op expr)
                    (eval* (fetch-Args expr) env)))))
    (define (eval* expr* env)
        (if (null? expr*) 
            '()
            (cons (eval (car expr*) env)
                    (eval* (cdr expr*) env))))
\end{lstlisting}
\tab Here $\code{expr}$ is the available representation of the interpreted expression, $\code{mem}$ is a delayed memory state. In the monovariant binding time analysis, we have to consider the value of the $\code{eval}$ function as always delayed, since in the $\code{isVar?}$ clause it is assigned the value of the delayed mapping $\code{mem}$. Hence, interpreting the expression $\code{(x + 3) * (7 - 2)}$ yields the deferred code 
\begin{lstlisting}
    (apply-Op '*
        (cons
            (apply-Op '+
                (cons 
                    (lookup 'x env)
                    (cons 3 '())))
            (cons 
                (apply-op '-
                    (cons 7 (cons 2 '())))
                '())))
\end{lstlisting}
\tab Our problem is that the second addition is not executed even though both arguments $7$ and $2$ are known. Thus, such a mixed interpreter does not allow for performing permissible computations, which is essentially its purpose. \newline
\tab It's possible to automatically transform the $\code{eval}$ function to the following form. The $\code{dbt-eval}$ function is used to determine the availability of the expression and the $\code{eval-static}$ function to calculate the fully accessible expression \cite{bulyonkov1993extracting}:
\begin{lstlisting}
(define (eval expr env)
    (cond
        ((isCst? expr) (fetch-Cst expr))
        ((isVar? expr) (lookup (fetch-Var expr) env))
        ((isOp? expr)
            (apply-Op
                (fetch-Op expr)
                (if (eq? (dbt-eval* (fetch-Args expr))
                    'static)
                    (static-eval* (fetch-Args expr))
                    (eval* (fetch-Argse) env))))))
\end{lstlisting}
\tab \textit{The} $\code{dbt-eval}$ \textit{function} annotates expression as static or dynamic:
\begin{lstlisting}
(define (dbt-eval expr)
    (cond
        ((isCst? expr) 'static)
        ((isVar? expr) 'dynamic)
        ((isOp? expr)
            (bt-apply-Op
            'static
            (dbt-eval* (fetch-Argu expr))))))
\end{lstlisting}
\tab \textit{The} $\code{static-eval}$ \textit{function} evaluates an expression that is known to be static:
\begin{lstlisting}
(define (static-eval expr)
    (cond
        ((isCst? expr) (fetch-Cst expr))
        ((isVar7 expr) (lookup (fetch-Var expr) env))
        ((isOp? expr)
            (apply-Op
                (fetch-Op expr)
                (static-eval* (fetch-Argue))))))
\end{lstlisting}
\tab \textit{The} $\code{eval*}$ \textit{function} performs binding time analysis on a list of expressions $\code{expr*}$ and applies $\code{static-eval}$ where possible to reduce expressions. 
\begin{lstlisting}
(define (eval* expr* env)
    (if (null? expr*)
        '()
        (cons
            (if (eq? (dbt-eval (car expr*)) 'static)
                (static-eval (car expr*))
                (eval (car expr*) env)
            (if (eq? (dbt-eval* (cdr expr*)) 'static)
                (static-eval* (cdr expr*))
                (eval* (cdr expr*) env))))
\end{lstlisting}
\tab \textit{The} $\code{dbt-eval*}$ \textit{function} annotates a list of expressions $\code{expr*}$, marking the empty list as static:
\begin{lstlisting}
(define (dbt-eval* expr*)
    (if (null? expr*)
    'static
    (bt-cons (dbt-eval (car expr*))
    (dbt-eval* (cdr expr*)))))
\end{lstlisting}
\tab \textit{Finally, the} $\code{static-eval*}$ \textit{function} applies static evaluation on a list of expressions $\code{expr*}$:
\begin{lstlisting}
(define (static-eval* expr*)
    (if (null? expr*)
    '()
    (cons   (static-eval (car expr*))
            (static-eval* (cdr expr*)))))
\end{lstlisting}
\tab While we have a larger code as a result, and the $\code{eval}$ function performs checks that do not affect the result, the $\code{dbt-eval}$ and $\code{eval-static}$ functions are fully ground and reduced during the specialization stage, resulting in a better object code for the same example:
\begin{lstlisting}
(apply-Op '*
    (cons
        (apply-Op '+
            (cons
                (lookup 'x env)
                '(3)))
        '(5)))
\end{lstlisting}
\tab Already it is evident that automatic binding time analysis can be quite challenging and suboptimal. For programs more complex than a calculator, it might indeed be more advantageous to give the binding time annotation power to the programmer, as it is done in staged miniKanren.
\subsection{Different types of partial evaluation}
Programs can be specialized using online and offline partial evaluation. With online partial evaluation, we decide to reduce an expression and perform the reduction at the same time. The offline technique involves annotating code using binding-time analysis, and performing the program transformations later \cite{christensen2004offline}. The accuracy of BTA depends on whether it is monovariant and polyvariant: the former assigns terms the safest binding time, or the join of all binding times involved, which may result in a loss of data, while the latter is more precise and flexible.
Finally, there is manual binding-time analysis, used by evaluators such as Pink \cite{amin2017collapsing}, which allows one to annotate programs by hand. Manual binding time analysis is used in Staged miniKanren to lift unifications by hand.
\subsection{Lightweight Modular Staging - an alternative staging method}
\tab Quasi-quotation is commonly used to distinguish between the parts of the computation that need to be performed now and those that are to be deferred to later stages.
Lightweight modular staging \cite{rompf2012lightweight}  uses an alternative approach where types represent different binding times. Thus, for a type $\code{T}$, we write $\texttt{Rep[T]}$ to denote a \textit{representation} of $\code{T}$. A term of type $\texttt{Rep[T]}$ is not computed yet, however we know that the result of the future computation
will have type $\code{T}$. An advantage of this approach is easier staging of practical algorithms such as the Fast Fourier Transform. \newline
\tab Another use case is specializing matrix multiplication to a matrix, where the result of the staging depends on how dense the matrix is. In particular, assume the matrix is known and the vector is unknown. Suppose the matrix is all zeros. Then instead of iterating through every row we can just have our specialized program return zero. Now suppose only the first row is non-zero. Then our specialized program still does not need to iterate through every row, as we can simply left-multiply the vector by the first row. On the other hand, if most entries in the matrix are non-zero, such a simplification might not be advantageous. This kind of specialization (multiplication with a known matrix and an unknown vector) is applicable in Markov models. Note that this process can be optimized using other staging systems as well. 
\subsection{Using supercompilation techniques for miniKanren.}
\tab One possible direction for staging is supercompilation, which is a method for creating an efficient residual program given partial input. While supercompilation includes partial evaluation, it involves a deeper transformation of the original program \cite{turchin1986concept}. The basic idea is as follows. We view a program as a computing machine with certain states, or stages, and configurations, or sets of states. We have a basic language for states and an extended language for representing states. Both states and stages are described by expressions. These can be \textit{ground}, if they describe precise states, or \textit{nonground}, if they describe states representing precise states. A supercompiler then analyses computation histories and attempts to generalize them via the extended language. 
\tab Supercompilation involves a transition from the basic system to a metasystem whose object of study is the basic system. A \textit{metacode} is a mapping $M$ from general expressions in the extended language to expressions in the metasystem having two properties:
\begin{enumerate}
    \item $M$ is homomorphic, in the sense that $M(E_1E_2) = M(E_1) M(E_2)$ where $E_1$ and $E_2$ are concatenated expressions.
    \item $M$ is injective, meaning no two expressions map to the same object expression.
\end{enumerate}
\tab Metacoded $E$ is denoted $M(E) = \mu E$ and, since $M$ is injective, we can $\textit{demetacode}$ an expression, written $\mu^{-1}E$. \newline 
\tab The generalized history is represented by a graph of states and transitions. The goal is then to perform transformations on the graph, such as reducing configurations, generalizing two configurations, and constructing subgraphs. A $\textit{strategy}$ is an algorithm for deciding which transformation needs to be taken at every moment during the creation of the graph. 
The possibility of applying supercompilation techniques in miniKanren has been explored in \cite{kuklinasupercompilation}.
\nocite*
\bibliographystyle{unsrt}
\bibliography{sample}
\end{document}